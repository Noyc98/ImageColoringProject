{"cells":[{"cell_type":"markdown","metadata":{"id":"wy0V8sV2iNtw"},"source":["Submitted by:\n","\n","* Michal Marom- 207479940\n","* Noy Cohen- 206713307\n","* David Sonego- 201416757\n"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":57074,"status":"ok","timestamp":1713710047121,"user":{"displayName":"Michal Marom","userId":"02122002056475035461"},"user_tz":-180},"id":"RcoUdDD3iSbC","outputId":"b6818e5a-636e-4793-9542-f7c672a00f2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive\n"]}],"source":["## Mounte drive\n","from google.colab import drive\n","import numpy as np\n","drive.mount('/content/gdrive')"]},{"cell_type":"markdown","metadata":{"id":"SSP0m12REptX"},"source":["## Hyper Parameters"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1713710047121,"user":{"displayName":"Michal Marom","userId":"02122002056475035461"},"user_tz":-180},"id":"VC098DXYEuBJ"},"outputs":[],"source":["BATCH_SIZE = 16\n","EPOCHS = 100\n","LEARNING_RATE = 0.0001\n","LR = 0.0001\n","# note_book_save_path = '/content/gdrive/MyDrive/Colab Notebooks/ImageColoringProject'\n","note_book_save_path = '/content/gdrive/MyDrive/ImageColoringProject'"]},{"cell_type":"markdown","metadata":{"id":"xQ7k2mEZiz4r"},"source":["## Imports"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aJxm8v2P7dvs"},"outputs":[],"source":["import pickle\n","from torch.utils.data import Subset\n","import random\n","import shutil\n","from torchvision.datasets import ImageFolder\n","import cv2\n","import os\n","import torch\n","import glob\n","import time\n","import numpy as np\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","import torch\n","from torch import nn, optim\n","from torchvision import transforms\n","from torchvision.utils import make_grid\n","from torch.utils.data import Dataset, DataLoader\n","import torchvision as tv\n","from torchvision.datasets import Flowers102\n","from PIL import Image\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","use_colab = None"]},{"cell_type":"markdown","metadata":{"id":"1nRK-gIaikuu"},"source":["## Data Loader"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"HK8EDM-qihOm"},"outputs":[],"source":["def data_loader(color_mode='gray', batch_size=32):\n","    # Define transformations for RGB images\n","    rgb_transform = transforms.Compose([\n","        transforms.Resize((256, 256)),  # Resize images to a fixed size\n","        transforms.ToTensor(),  # Convert images to PyTorch tensors\n","        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize images\n","    ])\n","\n","    # Loading the RGB dataset\n","    rgb_train = Flowers102(root='./data', download=True, split='test', transform=rgb_transform)\n","    rgb_test = Flowers102(root='./data', split='train', transform=rgb_transform)\n","    rgb_val = Flowers102(root='./data', split=\"val\", transform=rgb_transform)\n","\n","    # Creating RGB dataloaders\n","    train_loader_rgb = torch.utils.data.DataLoader(rgb_train, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n","    eval_loader_rgb = torch.utils.data.DataLoader(rgb_val, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n","    test_loader_rgb = torch.utils.data.DataLoader(rgb_test, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n","\n","    data = (train_loader_rgb, eval_loader_rgb, test_loader_rgb)\n","    return data"]},{"cell_type":"markdown","metadata":{"id":"8XNr7mt3jREp"},"source":["## Unet and Gan"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"lvxTTjEyjQNa"},"outputs":[],"source":["class conv_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.conv1 = nn.Conv2d(in_c, out_c, kernel_size=3, padding=1)\n","        self.bn1 = nn.BatchNorm2d(out_c)\n","        self.conv2 = nn.Conv2d(out_c, out_c, kernel_size=3, padding=1)\n","        self.bn2 = nn.BatchNorm2d(out_c)\n","        self.relu = nn.ReLU()\n","\n","    def forward(self, inputs):\n","        x = self.conv1(inputs)\n","        x = self.bn1(x)\n","        x = self.relu(x)\n","        x = self.conv2(x)\n","        x = self.bn2(x)\n","        x = self.relu(x)\n","        return x\n","\n","\n","class encoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.conv = conv_block(in_c, out_c)\n","        self.pool = nn.MaxPool2d((2, 2))\n","\n","    def forward(self, inputs):\n","        x = self.conv(inputs)\n","        p = self.pool(x)\n","        return x, p\n","\n","\n","class decoder_block(nn.Module):\n","    def __init__(self, in_c, out_c):\n","        super().__init__()\n","        self.up = nn.ConvTranspose2d(in_c, out_c, kernel_size=2, stride=2, padding=0)\n","        self.conv = conv_block(out_c+out_c, out_c)\n","    def forward(self, inputs, skip):\n","        x = self.up(inputs)\n","        x = torch.cat([x, skip], dim=1)\n","        x = self.conv(x)\n","        return x\n","\n","\n","# Define Generator (U-Net) architecture with VGG blocks\n","class UNetGenerator(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        \"\"\" Encoder \"\"\"\n","        self.e1 = encoder_block(1, 64)\n","        self.e2 = encoder_block(64, 128)\n","        self.e3 = encoder_block(128, 256)\n","        self.e4 = encoder_block(256, 512)\n","        \"\"\" Bottleneck \"\"\"\n","        self.b = conv_block(512, 1024)\n","        \"\"\" Decoder \"\"\"\n","        self.d1 = decoder_block(1024, 512)\n","        self.d2 = decoder_block(512, 256)\n","        self.d3 = decoder_block(256, 128)\n","        self.d4 = decoder_block(128, 64)\n","        \"\"\" Classifier \"\"\"\n","        self.outputs = nn.Conv2d(64, 3, kernel_size=1, padding=0)\n","\n","    def forward(self, inputs):\n","        \"\"\" Encoder \"\"\"\n","        s1, p1 = self.e1(inputs)\n","        s2, p2 = self.e2(p1)\n","        s3, p3 = self.e3(p2)\n","        s4, p4 = self.e4(p3)\n","        \"\"\" Bottleneck \"\"\"\n","        b = self.b(p4)\n","        \"\"\" Decoder \"\"\"\n","        d1 = self.d1(b, s4)\n","        d2 = self.d2(d1, s3)\n","        d3 = self.d3(d2, s2)\n","        d4 = self.d4(d3, s1)\n","        \"\"\" Classifier \"\"\"\n","        outputs = self.outputs(d4)\n","        return outputs\n","\n","class Critic(nn.Module):\n","    def __init__(self):\n","        super(Critic, self).__init__()\n","\n","        self.conv1 = nn.Sequential(\n","            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        self.conv2 = nn.Sequential(\n","            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n","            nn.InstanceNorm2d(128),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        self.conv3 = nn.Sequential(\n","            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n","            nn.InstanceNorm2d(256),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        self.conv4 = nn.Sequential(\n","            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n","            nn.InstanceNorm2d(512),\n","            nn.LeakyReLU(0.2, inplace=True)\n","        )\n","        self.conv5 = nn.Sequential(\n","            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1)\n","        )\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.conv2(x)\n","        x = self.conv3(x)\n","        x = self.conv4(x)\n","        x = self.conv5(x)\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"9tfGG5Y9juKB"},"source":["## Plots"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"hzc_qlkRjt1t"},"outputs":[],"source":["def plot_graph(loss, title, x_label='Batch', y_label='Loss'):\n","    plt.cla()\n","    plt.plot(range(len(loss)), loss, label=title)\n","    plt.xlabel(f'{x_label} Steps - axis')\n","    plt.ylabel(f'{y_label} Value - axis')\n","    plt.title(title)\n","\n","    plt.legend()\n","    plt.savefig(f'{note_book_save_path}/results/graph_{title}')\n","    plt.close()\n","    return"]},{"cell_type":"markdown","metadata":{"id":"PaaVqRPLswEH"},"source":["## PSNR accuracy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aZ5qGxm9syzn"},"outputs":[],"source":["def prepare_to_save_image(image):\n","    image_np = image.permute(1, 2, 0).detach().cpu().numpy()\n","    return (image_np - image_np.min()) / (image_np.max() - image_np.min())\n","\n","\n","def make_subplot(rbg_image, grey_image, gen_image):\n","    # Plot images\n","    fig, axs = plt.subplots(1, 3, figsize=(15, 5))\n","\n","    axs[0].imshow(rbg_image)\n","    axs[0].axis('off')\n","\n","    axs[1].imshow(grey_image.squeeze(), cmap='gray')\n","    axs[1].axis('off')\n","\n","    axs[2].imshow(gen_image)\n","    axs[2].axis('off')\n","\n","    plt.tight_layout()\n","    return plt\n","\n","\n","# Computes the PSNR between the input and target images.\n","def psnr(input_image, target_image):\n","    mse = torch.mean((input_image - target_image) ** 2)\n","    psnr_val = 10 * torch.log10(1 / mse)\n","    return psnr_val.detach().numpy()\n","\n","\n","def compute_psnr(loader_gray, loader_rgb, model_handler):\n","    psnr_values = []\n","    with torch.no_grad():\n","        for (gray_images, _), (rgb_images, _) in zip(loader_gray, loader_rgb):\n","            gray_images = gray_images.to(\"cuda\")\n","            rgb_images = rgb_images.to(\"cuda\")\n","            gen_images = model_handler.generator(gray_images)\n","            psnr_values.extend([psnr(gen_img, rgb_img) for gen_img, rgb_img in zip(gen_images.to(\"cpu\"), rgb_images.to(\"cpu\"))])\n","    avg_psnr = sum(psnr_values) / len(psnr_values)\n","    return avg_psnr"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"66NVU_1vPZoS"},"outputs":[],"source":["def convert_to_greyscale_batch(image_batch, grey_dir=None):\n","    # Ensure image_batch is a tensor\n","    if not torch.is_tensor(image_batch):\n","        image_batch = torch.tensor(image_batch)\n","\n","    # Check if the input is in [B, C, H, W] format\n","    if len(image_batch.shape) != 4:\n","        raise ValueError(\"Input image_batch should be in [B, C, H, W] format.\")\n","\n","    # Convert the batch of colored images to greyscale by taking the mean along the channel dimension\n","    grey_batch = torch.mean(image_batch, dim=1, keepdim=True)  # Assuming image_batch is in [B, C, H, W] format\n","\n","    return grey_batch\n","\n","def average_every_n_epochs(data, n=5):\n","    num_epochs = len(data)\n","    num_batches = num_epochs // n\n","    averaged_data = []\n","    for i in range(num_batches):\n","        start = i * n\n","        end = (i + 1) * n\n","        averaged_data.append(np.mean(data[start:end]))\n","    return averaged_data"]},{"cell_type":"markdown","metadata":{"id":"vzkL7ycfjad8"},"source":["## Model Handler"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1yAGoVsSjaOc"},"outputs":[],"source":["class ModelHandler:\n","    def __init__(self, train_loader_rgb, eval_loader_rgb, test_loader_rgb,\n","                 batch_size=BATCH_SIZE, num_epochs=EPOCHS, lr_G=LR, lr_C=LR, num_epochs_pre=EPOCHS):\n","        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","        self.num_epochs = num_epochs\n","        self.num_epochs_pre = num_epochs_pre\n","        self.lr_G = lr_G\n","        self.lr_C = lr_C\n","        self.batch_size = batch_size\n","        self.train_loader_rgb = train_loader_rgb\n","        self.eval_loader_rgb = eval_loader_rgb\n","        self.test_loader_rgb = test_loader_rgb\n","        self.MSEcriterion = nn.MSELoss()\n","        # WGAN\n","        self.generator = UNetGenerator().to(self.device)\n","        self.Critic = Critic().to(self.device)\n","        self.optimizer_G = optim.Adam(self.generator.parameters(), lr=self.lr_G, betas=(0, 0.9))\n","        self.optimizer_C = optim.Adam(self.Critic.parameters(), lr=self.lr_C, betas=(0, 0.9))\n","        # self.note_book_save_path = '/content/gdrive/MyDrive/Colab Notebooks/ImageColoringProject'\n","        self.note_book_save_path = note_book_save_path\n","\n","    def pretrain_generator(self):\n","        pretrained_model_path = f'{self.note_book_save_path}/saved_models/pretrained_model.pth'\n","        if os.path.exists(pretrained_model_path):\n","            self.generator.load_state_dict(torch.load(pretrained_model_path))\n","            print(\"Finished loading the previous pretrained model\")\n","        else:\n","            print(\"Starting to pretrain the generator!\")\n","\n","        accuracy, g_loss_per_epoch, avg_psnr_per_epoch = self.load_pretrained_arrays()\n","\n","        print(\"Starts to pretrain!\")\n","        self.generator.train()\n","        for epoch in range(len(accuracy), self.num_epochs_pre):\n","            psnr_values = []\n","            g_loss_per_batch = []\n","            for batch_idx, (rgb_images, _) in enumerate(self.train_loader_rgb):\n","\n","                rgb_images = rgb_images.to(self.device)\n","                gray_images = convert_to_greyscale_batch(rgb_images).to(self.device)\n","\n","                gen_images = self.generator(gray_images)\n","                loss = self.MSEcriterion(gen_images, rgb_images)\n","\n","                self.optimizer_G.zero_grad()\n","                loss.backward()\n","                self.optimizer_G.step()\n","\n","                if batch_idx % 20 == 0:\n","                    self.save_pretrained_images(gen_images, gray_images, rgb_images, epoch, batch_idx)\n","                    g_loss_per_batch.append(loss)\n","\n","\n","                psnr_values.extend([psnr(gen_img, rgb_img) for gen_img, rgb_img in zip(gen_images.to(\"cpu\"), rgb_images.to(\"cpu\"))])\n","                print(\"[Epoch %d/%d] [Batch %d/%d] [G loss: %f]\" % (\n","                    epoch, self.num_epochs_pre, batch_idx, len(self.train_loader_rgb), loss.item()))\n","\n","                rgb_images = rgb_images.to(\"cpu\")\n","                gen_images = gen_images.to(\"cpu\")\n","                gray_images = gray_images.to(\"cpu\")\n","\n","            # Calculate the average PSNR over all images\n","            avg_psnr = sum(psnr_values) / len(psnr_values)\n","            avg_psnr_per_epoch.append(avg_psnr)\n","            accuracy.append(avg_psnr)\n","            g_loss_per_epoch.append(np.average([l.item() for l in g_loss_per_batch]))\n","\n","            print(\"[Epoch: %d/%d] [g_loss_train: %f] [PSNR: %.2f dB]\" % (\n","                epoch, self.num_epochs_pre, np.average([l.item() for l in g_loss_per_batch]), avg_psnr))\n","            self.save_pretrained_model(pretrained_model_path)\n","            self.save_pretrained_arrays(accuracy, g_loss_per_epoch, avg_psnr_per_epoch)\n","\n","            # Clean up pretrain tensors\n","            torch.cuda.empty_cache()\n","            gc.collect()\n","\n","        return g_loss_per_epoch, accuracy, avg_psnr_per_epoch\n","\n","    def load_pretrained_arrays(self):\n","        save_dir = f'{self.note_book_save_path}/saved_models'\n","        os.makedirs(save_dir, exist_ok=True)\n","\n","        def load_array(filename):\n","            return list(np.load(filename)) if os.path.exists(filename) else []\n","\n","        return (\n","            load_array(f'{save_dir}/pretrained_accuracy.npy'),\n","            load_array(f'{save_dir}/pretrained_g_loss_per_epoch.npy'),\n","            load_array(f'{save_dir}/pretrained_avg_psnr_per_epoch.npy')\n","        )\n","\n","    def save_pretrained_images(self, gen_images, gray_images, rgb_images, epoch, batch_idx):\n","        os.makedirs(f'{self.note_book_save_path}/pre_trained_images', exist_ok=True)\n","        first_image_gen = prepare_to_save_image(gen_images[0])\n","        first_image_grey = prepare_to_save_image(gray_images[0])\n","        first_image_rbg = prepare_to_save_image(rgb_images[0])\n","        plt = make_subplot(first_image_rbg, first_image_grey, first_image_gen)\n","        plt.savefig(f'{self.note_book_save_path}/pre_trained_images/pre_trained_image_{epoch}_{batch_idx}.jpg')\n","        plt.close()\n","\n","    def save_pretrained_model(self, pretrained_model_path):\n","        torch.save(self.generator.state_dict(), pretrained_model_path)\n","\n","    def save_pretrained_arrays(self, accuracy, g_loss_per_epoch, avg_psnr_per_epoch):\n","        save_dir = f'{self.note_book_save_path}/saved_models'\n","        np.save(f'{save_dir}/pretrained_accuracy.npy', np.array(accuracy))\n","        np.save(f'{save_dir}/pretrained_g_loss_per_epoch.npy', np.array(g_loss_per_epoch))\n","        np.save(f'{save_dir}/pretrained_avg_psnr_per_epoch.npy', np.array(avg_psnr_per_epoch))\n","\n","    def test_model(self, loader_rgb):\n","        # Set model to eval mode\n","        self.generator.eval()\n","        self.Critic.eval()\n","        psnr_values_per_batch = []\n","\n","        for batch_idx, (rgb_images, _) in enumerate(loader_rgb):\n","            rgb_images = rgb_images.to(self.device)\n","            gray_images = convert_to_greyscale_batch(rgb_images).to(self.device)\n","\n","            # Generate rgb images\n","            gen_images = self.generator(gray_images)\n","\n","            # Calculate PSNR values for each generated image\n","            psnr_values_per_batch.extend([psnr(gen_img, rgb_img) for gen_img, rgb_img in zip(gen_images.detach().to(\"cpu\"), rgb_images.to(\"cpu\"))])\n","\n","            rgb_images = rgb_images.to(\"cpu\")\n","            gen_images = gen_images.to(\"cpu\")\n","            gray_images = gray_images.to(\"cpu\")\n","\n","        return psnr_values_per_batch\n","\n","    def val_model(self, loader_rgb):\n","        # Set model to eval mode\n","        self.generator.eval()\n","        self.Critic.eval()\n","        c_loss_per_batch = []\n","        g_loss_per_batch = []\n","        mse_loss_per_batch = []\n","\n","        for batch_idx, (rgb_images, _) in enumerate(loader_rgb):\n","            rgb_images = rgb_images.to(self.device)\n","            gray_images = convert_to_greyscale_batch(rgb_images).to(self.device)\n","\n","            # Calculate Generator loss\n","            gen_images = self.generator(gray_images)\n","            wgan_loss = -torch.mean(self.Critic(gen_images))\n","            mse_loss = self.MSEcriterion(rgb_images, gen_images)\n","            g_loss = wgan_loss * 0.3 + mse_loss * 0.7\n","\n","            # Calculate Critic loss\n","            c_loss = -torch.mean(self.Critic(rgb_images)) + torch.mean(self.Critic(gen_images.detach()))\n","\n","            # Save Critic and Generator loss per batch\n","            c_loss_per_batch.append(c_loss.item())\n","            g_loss_per_batch.append(g_loss.item())\n","            mse_loss_per_batch.append(mse_loss.item())\n","\n","\n","            rgb_images = rgb_images.to(\"cpu\")\n","            gen_images = gen_images.to(\"cpu\")\n","            gray_images = gray_images.to(\"cpu\")\n","\n","        c_loss_avr = sum(c_loss_per_batch)/len(c_loss_per_batch)\n","        g_loss_avr = sum(g_loss_per_batch)/len(g_loss_per_batch)\n","        mse_loss_avr = sum(mse_loss_per_batch) / len(mse_loss_per_batch)\n","\n","\n","        # Reset model to train mode\n","        self.generator.train()\n","        self.Critic.train()\n","\n","        return c_loss_avr, g_loss_avr, mse_loss_avr\n","\n","\n","    def results_visualization(self):\n","        counter = 0\n","        for batch_idx, (rgb_images, _) in enumerate(self.test_loader_rgb):\n","\n","            rgb_images = rgb_images.to(self.device)\n","            gray_images = convert_to_greyscale_batch(rgb_images).to(self.device)\n","\n","            # Generate RGB images from grayscale\n","            gen_images = self.generator(gray_images)\n","\n","            for idx, (gray_image, rgb_image, gen_image) in enumerate(zip(gray_images, rgb_images, gen_images)):\n","                gray_image_np = prepare_to_save_image(gray_image)\n","                gen_image_np = prepare_to_save_image(gen_image)\n","                rgb_image_np = prepare_to_save_image(rgb_image)\n","                plt = make_subplot(rgb_image_np, gray_image_np, gen_image_np)\n","\n","                plt.savefig(f'{self.note_book_save_path}/results/image_%d.jpg' % counter)\n","                plt.close()\n","                counter += 1\n","\n","            rgb_images = rgb_images.to(\"cpu\")\n","            gen_images = gen_images.to(\"cpu\")\n","            gray_images = gray_images.to(\"cpu\")\n","\n","    def gradient_penalty(self, real_images, fake_images):\n","        device = real_images.device\n","        alpha = torch.rand(real_images.size(0), 1, 1, 1, device=device)\n","        alpha.expand_as(real_images)\n","        interpolated = (alpha * real_images + (1 - alpha) * fake_images).requires_grad_(True)\n","        pred = self.Critic(interpolated)\n","        gradients = torch.autograd.grad(outputs=pred, inputs=interpolated,\n","                                        grad_outputs=torch.ones(pred.size(), device=device),\n","                                        create_graph=True, retain_graph=True)[0]\n","        gradients = gradients.view(gradients.size(0), -1)\n","        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n","        return gradient_penalty\n","\n","    def save_generated_images(self, gen_images, gray_images, rgb_images, epoch, batch_idx):\n","        os.makedirs(f'{self.note_book_save_path}/generated_images', exist_ok=True)\n","        first_image_gen = prepare_to_save_image(gen_images[0])\n","        first_image_grey = prepare_to_save_image(gray_images[0])\n","        first_image_rbg = prepare_to_save_image(rgb_images[0])\n","        plt = make_subplot(first_image_rbg, first_image_grey, first_image_gen)\n","        plt.savefig(f'{self.note_book_save_path}/generated_images/image_epoch_{epoch}batch{batch_idx}.jpg')\n","        plt.close()\n","\n","    def load_models(self):\n","        if os.path.exists(f'{self.note_book_save_path}/saved_models/generator_model.pth') and os.path.exists(\n","                f'{self.note_book_save_path}/saved_models/Critic_model.pth'):\n","            self.generator.load_state_dict(torch.load(f'{self.note_book_save_path}/saved_models/generator_model.pth'))\n","            self.Critic.load_state_dict(torch.load(f'{self.note_book_save_path}/saved_models/Critic_model.pth'))\n","            print(\"Finished loading the previous trained models!\")\n","        elif os.path.exists(f'{self.note_book_save_path}/saved_models/pretrained_model.pth'):\n","            self.generator.load_state_dict(torch.load(f'{self.note_book_save_path}/saved_models/pretrained_model.pth'))\n","            print(\"Finished loading the pretrained generator!\")\n","        else:\n","            print(\"Starting to train without pretrained model!\")\n","\n","    def save_arrays(self, c_loss_per_epoch, g_loss_per_epoch, accuracy, val_losses_g, val_losses_c, mse_losses_per_epoch, wgan_losses_per_epoch,val_losses_mse):\n","        np.save(f'{self.note_book_save_path}/saved_models/c_loss_per_epoch.npy', np.array(c_loss_per_epoch))\n","        np.save(f'{self.note_book_save_path}/saved_models/g_loss_per_epoch.npy', np.array(g_loss_per_epoch))\n","        np.save(f'{self.note_book_save_path}/saved_models/accuracy.npy', np.array(accuracy))\n","        np.save(f'{self.note_book_save_path}/saved_models/val_losses_g.npy', np.array(val_losses_g))\n","        np.save(f'{self.note_book_save_path}/saved_models/val_losses_c.npy', np.array(val_losses_c))\n","        np.save(f'{self.note_book_save_path}/saved_models/val_losses_mse.npy', np.array(val_losses_mse))\n","        np.save(f'{self.note_book_save_path}/saved_models/wgan_losses_per_epoch.npy', np.array(wgan_losses_per_epoch))\n","        np.save(f'{self.note_book_save_path}/saved_models/mse_losses_per_epoch.npy', np.array(mse_losses_per_epoch))\n","        torch.save(self.generator.state_dict(), f'{self.note_book_save_path}/saved_models/generator_model.pth')\n","        torch.save(self.Critic.state_dict(), f'{self.note_book_save_path}/saved_models/Critic_model.pth')\n","\n","    def load_arrays(self):\n","        def load_array(filename):\n","            return list(np.load(filename)) if os.path.exists(filename) else []\n","\n","        return(\n","            load_array(f'{self.note_book_save_path}/saved_models/c_loss_per_epoch.npy'),\n","            load_array(f'{self.note_book_save_path}/saved_models/g_loss_per_epoch.npy'),\n","            load_array(f'{self.note_book_save_path}/saved_models/accuracy.npy'),\n","            load_array(f'{self.note_book_save_path}/saved_models/val_losses_g.npy'),\n","            load_array(f'{self.note_book_save_path}/saved_models/val_losses_c.npy'),\n","            load_array(f'{self.note_book_save_path}/saved_models/val_losses_mse.npy'),\n","            load_array(f'{self.note_book_save_path}/saved_models/wgan_losses_per_epoch.npy'),\n","            load_array(f'{self.note_book_save_path}/saved_models/mse_losses_per_epoch.npy')\n","            # load_array(f'{self.note_book_save_path}/saved_models/test_accuracy.npy')\n","        )\n","\n","    def train(self):\n","        # Load previous trained models and arrays\n","        self.load_models()\n","\n","        # Initialize arrays\n","        c_loss_per_epoch = []\n","        g_loss_per_epoch = []\n","        accuracy = []\n","        val_losses_g = []\n","        val_losses_c = []\n","        val_losses_mse = []\n","        mse_losses_per_epoch = []\n","        wgan_losses_per_epoch = []\n","        train_critic = False\n","\n","        # Load arrays\n","        # c_loss_per_epoch, g_loss_per_epoch, accuracy, val_losses_g, val_losses_c,val_losses_mse, wgan_losses_per_epoch, mse_losses_per_epoch, test_accuracy = self.load_arrays()\n","        c_loss_per_epoch, g_loss_per_epoch, accuracy, val_losses_g, val_losses_c,val_losses_mse, wgan_losses_per_epoch, mse_losses_per_epoch = self.load_arrays()\n","\n","        # Configure to train mode\n","        self.generator.train()\n","        self.Critic.train()\n","\n","        # Train the model\n","        for epoch in range(len(c_loss_per_epoch), self.num_epochs):\n","            psnr_values = []\n","            g_loss_per_batch = []\n","            c_loss_per_batch = []\n","            mse_losses_per_batch = []\n","            wgan_losses_per_batch = []\n","            for batch_idx, (rgb_images, _) in enumerate(self.train_loader_rgb):\n","\n","                rgb_images = rgb_images.to(self.device)\n","                gray_images = convert_to_greyscale_batch(rgb_images).to(self.device)\n","\n","                # Training the critic every 4 steps\n","                if batch_idx % 4 == 0 and batch_idx != 0:\n","                    train_critic = True\n","                    # Train the critic\n","                    gen_images = self.generator(gray_images)\n","                    self.optimizer_C.zero_grad()\n","                    loss_c = -torch.mean(self.Critic(rgb_images)) + torch.mean(self.Critic(gen_images.detach()))\n","                    gp = self.gradient_penalty(rgb_images, gen_images.detach())\n","                    loss_c += 10 * gp\n","                    loss_c.backward()\n","                    self.optimizer_C.step()\n","\n","                # Training the generator\n","                for param in self.Critic.parameters():\n","                    param.requires_grad = False\n","\n","                self.optimizer_G.zero_grad()\n","                gen_images = self.generator(gray_images)\n","                wgan_loss = -torch.mean(self.Critic(gen_images))\n","                mse_loss = self.MSEcriterion(rgb_images, gen_images)\n","                loss_g = wgan_loss * 0.3 + mse_loss * 0.7\n","                loss_g.backward()\n","                self.optimizer_G.step()\n","\n","                for param in self.Critic.parameters():\n","                    param.requires_grad = True\n","\n","                if train_critic:\n","                    c_loss_per_batch.append(loss_c)\n","                    g_loss_per_batch.append(loss_g)\n","                    mse_losses_per_batch.append(mse_loss.item())\n","                    wgan_losses_per_batch.append(wgan_loss.item())\n","\n","                    psnr_values.extend([psnr(gen_img, rgb_img) for gen_img, rgb_img in zip(gen_images.detach().to(\"cpu\"), rgb_images.to(\"cpu\"))])\n","\n","                    # Print loss\n","                    print(\"[Epoch %d/%d] [Batch %d/%d] [Critic loss: %f] [G loss: %f] [PSNR accuracy: %f] \"\n","                          % (epoch, self.num_epochs, batch_idx, len(self.train_loader_rgb), loss_c.item(),\n","                             loss_g.item(), sum(psnr_values) / len(psnr_values)))\n","\n","                    # Save generated images\n","                    self.save_generated_images(gen_images, gray_images, rgb_images, epoch, batch_idx)\n","                    train_critic = False\n","\n","                rgb_images = rgb_images.to(\"cpu\")\n","                gray_images = gray_images.to(\"cpu\")\n","                gen_images = gen_images.to(\"cpu\")\n","\n","                # Free up pretrain tensors\n","                torch.cuda.empty_cache()\n","\n","            # Update Validation losses arrays\n","            c_loss_val, g_loss_val, mse_loss_val  = self.val_model(self.eval_loader_rgb)\n","            val_losses_c.append(c_loss_val)\n","            val_losses_g.append(g_loss_val)\n","            val_losses_mse.append(mse_loss_val)\n","\n","            # Calculate model accuracy\n","            accuracy.append(sum(psnr_values) / len(psnr_values))\n","\n","            # Calculate loss per epoch\n","            g_loss_per_epoch.append(np.average([l.item() for l in g_loss_per_batch]))\n","            c_loss_per_epoch.append(np.average([l.item() for l in c_loss_per_batch]))\n","            mse_losses_per_epoch.append(np.average([l for l in mse_losses_per_batch]))\n","            wgan_losses_per_epoch.append(np.average([l for l in wgan_losses_per_batch]))\n","\n","            # Save arrays\n","            self.save_arrays(c_loss_per_epoch, g_loss_per_epoch, accuracy, val_losses_g, val_losses_c, mse_losses_per_epoch, wgan_losses_per_epoch, val_losses_mse)\n","\n","            # Free up pretrain tensors\n","            torch.cuda.empty_cache()\n","\n","        # Update Test losses arrays\n","        if not os.path.exists(f'{self.note_book_save_path}/saved_models/test_accuracy.pth'):\n","          test_accuracy = self.test_model(self.test_loader_rgb)\n","          np.save(f'{self.note_book_save_path}/saved_models/test_accuracy.npy', np.array(test_accuracy))\n","\n","        return c_loss_per_epoch, g_loss_per_epoch, accuracy, val_losses_g, val_losses_c, mse_losses_per_epoch, wgan_losses_per_epoch, test_accuracy, val_losses_mse\n","\n","    def return_arrays(self):\n","        c_loss_per_epoch, g_loss_per_epoch, accuracy, test_accuracy, val_accuracy, test_losses_g, val_losses_g, mse_losses_per_epoch, wgan_losses_per_epoch = self.load_arrays()\n","        return c_loss_per_epoch, g_loss_per_epoch, accuracy, test_accuracy, val_accuracy, test_losses_g, val_losses_g, mse_losses_per_epoch, wgan_losses_per_epoch\n"]},{"cell_type":"markdown","metadata":{"id":"s8T3C6FRj7T3"},"source":["## Main"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"wCGsUzRyBO7M"},"outputs":[{"name":"stdout","output_type":"stream","text":["|===========================================================================|\n","|                  PyTorch CUDA memory summary, device ID 0                 |\n","|---------------------------------------------------------------------------|\n","|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n","|===========================================================================|\n","|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n","|---------------------------------------------------------------------------|\n","| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |\n","|---------------------------------------------------------------------------|\n","| Allocations           |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Active allocs         |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| GPU reserved segments |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Non-releasable allocs |       0    |       0    |       0    |       0    |\n","|       from large pool |       0    |       0    |       0    |       0    |\n","|       from small pool |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize allocations  |       0    |       0    |       0    |       0    |\n","|---------------------------------------------------------------------------|\n","| Oversize GPU segments |       0    |       0    |       0    |       0    |\n","|===========================================================================|\n","\n"]}],"source":["import gc\n","torch.cuda.empty_cache()\n","gc.collect()\n","print(torch.cuda.memory_summary(device=None, abbreviated=False))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"Z8sEvfCfqjyF"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 344862509/344862509 [00:12\u003c00:00, 28659263.12it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data/flowers-102/102flowers.tgz to data/flowers-102\n","Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/imagelabels.mat to data/flowers-102/imagelabels.mat\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 502/502 [00:00\u003c00:00, 464594.13it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/setid.mat to data/flowers-102/setid.mat\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 14989/14989 [00:00\u003c00:00, 12353787.12it/s]"]},{"name":"stdout","output_type":"stream","text":["Finished data loading!\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["data = data_loader()\n","train_loader_rgb, eval_loader_rgb, test_loader_rgb = data\n","print(\"Finished data loading!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"ZQiXtr8uOFuT"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished ModelHandler!\n"]}],"source":["# Define and initialize your model handler\n","model_handler = ModelHandler(train_loader_rgb,eval_loader_rgb, test_loader_rgb,\n","                              batch_size=BATCH_SIZE, num_epochs=EPOCHS, lr_G=LR, lr_C=LR, num_epochs_pre=4)\n","print(\"Finished ModelHandler!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"8hmpTskWq1rw"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished loading the previous pretrained model\n","Starts to pretrain!\n","Elapsed time in seconds: 3.228003740310669\n"]}],"source":["# Train Model\n","start_time = time.time()\n","model_handler.pretrain_generator()\n","end_time = time.time()\n","# Calculate elapsed time in seconds\n","elapsed_time = end_time - start_time\n","print(\"Elapsed time in seconds:\", elapsed_time)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"aikuRgdEq01z"},"outputs":[{"name":"stdout","output_type":"stream","text":["Finished loading the previous trained models!\n","Elapsed time in seconds: 29.61925959587097\n"]}],"source":["# Define Time\n","start_time = time.time()\n","c_loss_per_epoch, g_loss_per_epoch, accuracy, val_losses_g, val_losses_c, mse_losses_per_epoch, wgan_losses_per_epoch, test_accuracy, val_losses_mse = model_handler.train()\n","end_time = time.time()\n","# Calculate elapsed time in seconds\n","elapsed_time = end_time - start_time\n","print(\"Elapsed time in seconds:\", elapsed_time)"]},{"cell_type":"markdown","metadata":{"id":"vNkswrwDAn4J"},"source":["## Save Graphs"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"5qr_LIjNj7iH"},"outputs":[],"source":["# model_handler.results_visualization()\n","# # plots\n","# plot_graph(test_accuracy, \"Test Accuracy\", x_label='Batch', y_label=\"Accuracy\")\n","# plot_graph(g_loss_per_epoch, \"Train Generator Loss\", x_label='Epoch', y_label='Loss')\n","# plot_graph(c_loss_per_epoch, \"Train Critic Loss\", x_label='Epoch', y_label='Loss')\n","# plot_graph(accuracy, \"Train Accuracy\", x_label='Epoch', y_label=\"Accuracy\")\n","# plot_graph(val_losses_g, \"Validation Generator Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(val_losses_c, \"Validation Critic Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(val_losses_mse, \"Validation MSE Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(mse_losses_per_epoch, \"Train MSE Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(wgan_losses_per_epoch, \"Train Wasserstein Loss\", x_label='Epoch', y_label=\"Loss\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"xGgbdQ8eRQy1"},"outputs":[],"source":["# Averaging every 5 epochs for each metric\n","\n","# test_accuracy_avg = average_every_n_epochs(test_accuracy, n=3)\n","# c_loss_per_epoch_avg = average_every_n_epochs(c_loss_per_epoch)\n","# g_loss_per_epoch_avg = average_every_n_epochs(g_loss_per_epoch)\n","# accuracy_avg = average_every_n_epochs(accuracy)\n","# val_losses_g_avg = average_every_n_epochs(val_losses_g)\n","# val_losses_c_avg = average_every_n_epochs(val_losses_c)\n","# val_losses_mse_avg = average_every_n_epochs(val_losses_mse, n=10)\n","# mse_losses_per_epoch_avg = average_every_n_epochs(mse_losses_per_epoch)\n","# wgan_losses_per_epoch_avg = average_every_n_epochs(wgan_losses_per_epoch)\n","\n","# Plotting the averaged data\n","# plot_graph(test_accuracy_avg, \"Averaged Test Accuracy\", x_label='Batch', y_label=\"Accuracy\")\n","# plot_graph(g_loss_per_epoch_avg, \"Averaged Train Generator Loss\", x_label='Epoch', y_label='Loss')\n","# plot_graph(c_loss_per_epoch_avg, \"Averaged Train Critic Loss\", x_label='Epoch', y_label='Loss')\n","# plot_graph(accuracy_avg, \"Averaged Train Accuracy\", x_label='Epoch', y_label=\"Accuracy\")\n","# plot_graph(val_losses_g_avg, \"Averaged Validation Generator Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(val_losses_c_avg, \"Averaged Validation Critic Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(val_losses_mse_avg, \"Averaged Validation MSE Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(mse_losses_per_epoch_avg, \"Averaged Train MSE Loss\", x_label='Epoch', y_label=\"Loss\")\n","# plot_graph(wgan_losses_per_epoch_avg, \"Averaged Train Wasserstein Loss\", x_label='Epoch', y_label=\"Loss\")"]},{"cell_type":"markdown","metadata":{"id":"KuJMpTdVNx0z"},"source":["## Run model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xEaH8QNxPwIs"},"outputs":[],"source":["accuracy = model_handler.test_model(test_loader_rgb)\n","print(f\"Test accuracy: {np.mean(accuracy)}\")\n","model_handler.results_visualization()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XwPYXJBKBfBC"},"outputs":[],"source":["from google.colab import runtime\n","runtime.unassign()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}